{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee50b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a751c2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label                                              tweet\n",
      "0   1      0   @user when a father is dysfunctional and is s...\n",
      "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
      "2   3      0                                bihday your majesty\n",
      "3   4      0  #model   i love u take with u all the time in ...\n",
      "4   5      0             factsguide: society now    #motivation\n",
      "5   6      0  [2/2] huge fan fare and big talking before the...\n",
      "6   7      0   @user camping tomorrow @user @user @user @use...\n",
      "7   8      0  the next school year is the year for exams.ð...\n",
      "8   9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
      "9  10      0   @user @user welcome here !  i'm   it's so #gr...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv (\"train.csv\")\n",
    "print (df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf808e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(tweet):\n",
    "    #membersihkan teks\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$\",\"\",tweet, flags = re.MULTILINE)\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\",\"\",tweet)\n",
    "    tweet = re.sub(r\"[^\\w\\s]\",\"\",tweet)\n",
    "    tweet = re.sub(r'ð','',tweet)\n",
    "    #token\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_tweet = [w for w in tweet_tokens if not w in stop_words]\n",
    "    return \" \".join(filtered_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3838711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweet = df[\"tweet\"].apply(data_processing) #buat manggil fungsi data_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53401859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  label                                              tweet\n",
      "0          1      0  father dysfunctional selfish drags kids dysfun...\n",
      "1          2      0  thanks lyft credit cant use cause dont offer w...\n",
      "2          3      0                                     bihday majesty\n",
      "3          4      0                        model love u take u time ur\n",
      "4          5      0                      factsguide society motivation\n",
      "...      ...    ...                                                ...\n",
      "31956  31957      0     fishing tomorrow carnt wait first time 2 years\n",
      "31957  31958      0                                    ate isz youuuâï\n",
      "31958  31959      0  see nina turner airwaves trying wrap mantle ge...\n",
      "31959  31960      0    listening sad songs monday morning otw work sad\n",
      "31961  31962      0                                       thank follow\n",
      "\n",
      "[28424 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates(\"tweet\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa71da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing(data):\n",
    "    data_tokenize = word_tokenize(data)\n",
    "    tweet = [lemmatizer.lemmatize(word) for word in data_tokenize]\n",
    "    return \" \".join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc812d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81ac5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        father dysfunctional selfish drag kid dysfunct...\n",
      "1        thanks lyft credit cant use cause dont offer w...\n",
      "2                                           bihday majesty\n",
      "3                              model love u take u time ur\n",
      "4                            factsguide society motivation\n",
      "                               ...                        \n",
      "31956        fishing tomorrow carnt wait first time 2 year\n",
      "31957                                      ate isz youuuâï\n",
      "31958    see nina turner airwave trying wrap mantle gen...\n",
      "31959       listening sad song monday morning otw work sad\n",
      "31961                                         thank follow\n",
      "Name: tweet, Length: 28424, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "290ac72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,3)).fit(df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "666638ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 357592\n",
      "\n",
      "First 20 features: \n",
      "['0000001' '0000001 polluting' '0000001 polluting niger' '00027'\n",
      " '00027 photooftheday' '00027 photooftheday music' '001' '0035' '00h30'\n",
      " '01' '01 4995' '01 4995 rustic' '01 7900' '01 7900 shopalyssas' '01 blog'\n",
      " '01 blog silver' '01 croatia' '01 croatia happy' '01 may' '01 may actual']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"Number of features: {}\\n\".format(len(feature_names)))\n",
    "print(\"First 20 features: \\n{}\".format(feature_names[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "085ea343",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tweet']\n",
    "Y = df['label']\n",
    "V = vect.transform(X)\n",
    "\n",
    "Tfidf_Save = \"Tfidf.pkl\"  \n",
    "\n",
    "with open(Tfidf_Save, \"wb\") as file:  \n",
    "    pickle.dump(vect, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11357371",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(V, Y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb636a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of x_train: (19896, 357592)\n",
      "Size of y_train: (19896,)\n",
      "Size of x_test:  (8528, 357592)\n",
      "Size of y_test:  (8528,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of x_train:\", (x_train.shape))\n",
    "print(\"Size of y_train:\", (y_train.shape))\n",
    "print(\"Size of x_test: \", (x_test.shape))\n",
    "print(\"Size of y_test: \", (y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e14fd77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "SVM.fit(x_train, y_train)\n",
    "\n",
    "y_pred = SVM.predict(x_test)\n",
    "\n",
    "Pkl_Filename = \"SVM_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(SVM, file)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec09b2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7937,    8],\n",
       "       [ 454,  129]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d52b8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      7945\n",
      "           1       0.94      0.22      0.36       583\n",
      "\n",
      "    accuracy                           0.95      8528\n",
      "   macro avg       0.94      0.61      0.67      8528\n",
      "weighted avg       0.95      0.95      0.93      8528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
